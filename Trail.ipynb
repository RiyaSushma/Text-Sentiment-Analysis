{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "208e0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import stopwords as stopwords\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import preprocessor as p\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import pygsheets\n",
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "import pycountry\n",
    "from tkinter import messagebox\n",
    "from tkinter import ttk\n",
    "import webbrowser\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "b1ce4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"C:/Users/lenovo/Desktop/Project - Major/Data_for_train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "a78e6e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_sentiment_label</th>\n",
       "      <th>Tweets_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>@SouthwestAir I am scheduled for the morning, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@SouthwestAir seeing your workers time in and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>@united Flew ORD to Miami and back and  had gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@SouthwestAir @dultch97 that's horse radish ðŸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@united so our flight into ORD was delayed bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260270</th>\n",
       "      <td>neutral</td>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260271</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260272</th>\n",
       "      <td>negative</td>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260273</th>\n",
       "      <td>negative</td>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260274</th>\n",
       "      <td>positive</td>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_sentiment_label                                 Tweets_description\n",
       "0                  negative  @SouthwestAir I am scheduled for the morning, ...\n",
       "1                  positive  @SouthwestAir seeing your workers time in and ...\n",
       "2                  positive  @united Flew ORD to Miami and back and  had gr...\n",
       "3                  negative  @SouthwestAir @dultch97 that's horse radish ðŸ...\n",
       "4                  negative  @united so our flight into ORD was delayed bec...\n",
       "...                     ...                                                ...\n",
       "260270              neutral  why these 456 crores paid neerav modi not reco...\n",
       "260271              neutral  dear rss terrorist payal gawar what about modi...\n",
       "260272             negative  did you cover her interaction forum where she ...\n",
       "260273             negative  there big project came into india modi dream p...\n",
       "260274             positive  have you ever listen about like gurukul where ...\n",
       "\n",
       "[260275 rows x 2 columns]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text_sentiment_label', 'Tweets_description']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "292d0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "   ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "b991258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_mentions(text):\n",
    "    # remove mentions (words that start with '@')\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    # Remove non-ASCII characters\n",
    "    tweet = remove_mentions(tweet)\n",
    "    tweet = re.sub('[^\\x00-\\x7F]+', ' ', tweet)\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize tweet\n",
    "    word_tokens = word_tokenize(tweet.lower())\n",
    "    \n",
    "    # Lemmatize words and remove stop words, emoticons, and punctuation\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words and w not in string.punctuation:\n",
    "            w = re.sub(r'@\\w+', '', w)\n",
    "            filtered_tweet.append(w)\n",
    "\n",
    "    return ' '.join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "dec93cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.read_excel(\"C:/Users/lenovo/Desktop/Streamlit tutorial/Text_Sentiment_analysis/features.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "561a4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lowercase_tokens = [token.lower() for token in tokens]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in lowercase_tokens]\n",
    "    unique_tokens = list(set(lemmatized_tokens))\n",
    "    processed_text = \" \".join(unique_tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "58b29172",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature['Feeling'] = feature['Feeling'].apply(str)\n",
    "feature['Feeling'] = feature['Feeling'].apply(process_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "3e919edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absorbed',\n",
       " 'beguiled',\n",
       " 'busy',\n",
       " 'charmed',\n",
       " 'cheered-up',\n",
       " 'convulsed',\n",
       " 'delighted',\n",
       " 'diverted',\n",
       " 'engaged',\n",
       " 'entertained',\n",
       " 'glad',\n",
       " 'interested',\n",
       " 'involved',\n",
       " 'pleased',\n",
       " 'preoccupied',\n",
       " 'regaled',\n",
       " 'wowed',\n",
       " 'beaming',\n",
       " 'blessed',\n",
       " 'blissful',\n",
       " 'carefree',\n",
       " 'cheerful',\n",
       " 'confident',\n",
       " 'content',\n",
       " 'euphoric',\n",
       " 'exhilarated',\n",
       " 'glowing',\n",
       " 'gratified',\n",
       " 'inclined',\n",
       " 'joy',\n",
       " 'powerful',\n",
       " 'radiant',\n",
       " 'secure',\n",
       " 'self-accepting',\n",
       " 'strong',\n",
       " 'thrilled',\n",
       " 'amicable',\n",
       " 'anxiety-free',\n",
       " 'calm',\n",
       " 'committed',\n",
       " 'composed',\n",
       " 'cordial',\n",
       " 'flexible',\n",
       " 'harmonious',\n",
       " 'isolated',\n",
       " 'patient',\n",
       " 'pleasant',\n",
       " 'productive',\n",
       " 'quiet',\n",
       " 'relaxed',\n",
       " 'restful',\n",
       " 'serene',\n",
       " 'soothed',\n",
       " 'stable',\n",
       " 'tranquil',\n",
       " 'undisturbed',\n",
       " 'untroubled',\n",
       " 'unworried',\n",
       " 'acceptable',\n",
       " 'amused',\n",
       " 'contented',\n",
       " 'elated',\n",
       " 'good',\n",
       " 'grateful',\n",
       " 'happy',\n",
       " 'lovely',\n",
       " 'nice',\n",
       " 'overjoyed',\n",
       " 'satisfied',\n",
       " 'self-approving',\n",
       " 'superior',\n",
       " 'thankful',\n",
       " 'well-pleased',\n",
       " 'athletic',\n",
       " 'buff',\n",
       " 'compelling',\n",
       " 'controlling',\n",
       " 'dominant',\n",
       " 'dynamic',\n",
       " 'hard',\n",
       " 'herculean',\n",
       " 'high-powered',\n",
       " 'important',\n",
       " 'influential',\n",
       " 'intoxicating',\n",
       " 'irresistible',\n",
       " 'manly',\n",
       " 'mighty',\n",
       " 'persuasive',\n",
       " 'ripped',\n",
       " 'shredded',\n",
       " 'solid',\n",
       " 'spirituous',\n",
       " 'stiff',\n",
       " 'sturdy',\n",
       " 'thunderous',\n",
       " 'tough',\n",
       " 'vigorous',\n",
       " 'well-built',\n",
       " 'affectionate',\n",
       " 'amorous',\n",
       " 'beautiful',\n",
       " 'charming',\n",
       " 'dreamy',\n",
       " 'erotic',\n",
       " 'fond',\n",
       " 'generous',\n",
       " 'humble',\n",
       " 'idealistic',\n",
       " 'illicit',\n",
       " 'intimate',\n",
       " 'kind',\n",
       " 'lovable',\n",
       " 'lovesick',\n",
       " 'loving',\n",
       " 'lustful',\n",
       " 'passionate',\n",
       " 'romanticist',\n",
       " 'tolerant',\n",
       " 'annoyed',\n",
       " 'bad-tempered',\n",
       " 'conceited',\n",
       " 'crabby',\n",
       " 'cross',\n",
       " 'disgusted',\n",
       " 'displeased',\n",
       " 'dissatisfied',\n",
       " 'enraged',\n",
       " 'exasperated',\n",
       " 'fuming',\n",
       " 'furious',\n",
       " 'hot-tempered',\n",
       " 'hot-headed',\n",
       " 'intolerant',\n",
       " 'irritated',\n",
       " 'jealous',\n",
       " 'mad',\n",
       " 'outraged',\n",
       " 'provoked',\n",
       " 'raging',\n",
       " 'resentful',\n",
       " 'stingy',\n",
       " 'waspish',\n",
       " 'wrathful',\n",
       " 'bummer',\n",
       " 'disinterested',\n",
       " 'dull',\n",
       " 'fatigued',\n",
       " 'inattentive',\n",
       " 'flat',\n",
       " 'lifeless',\n",
       " 'monotonous',\n",
       " 'spiritless',\n",
       " 'stale',\n",
       " 'stodgy',\n",
       " 'stuffy',\n",
       " 'stupid',\n",
       " 'tamed',\n",
       " 'tedious',\n",
       " 'tired',\n",
       " 'tiresome',\n",
       " 'tiring',\n",
       " 'appalled',\n",
       " 'dismayed',\n",
       " 'embarrassed',\n",
       " 'grossed-out',\n",
       " 'horrified',\n",
       " 'nasty',\n",
       " 'nauseated',\n",
       " 'offended',\n",
       " 'repelled',\n",
       " 'repulsed',\n",
       " 'revolted',\n",
       " 'shocked',\n",
       " 'sickened',\n",
       " 'trashed',\n",
       " 'ugly',\n",
       " 'unwanted',\n",
       " 'used',\n",
       " 'violated',\n",
       " 'sad',\n",
       " 'angry',\n",
       " 'afraid',\n",
       " 'confused',\n",
       " 'anxious',\n",
       " 'nervous',\n",
       " 'excited',\n",
       " 'depressed',\n",
       " 'lonely',\n",
       " 'hopeful',\n",
       " 'joyful',\n",
       " 'empowered',\n",
       " 'inspired',\n",
       " 'frustrated',\n",
       " 'overwhelmed',\n",
       " 'guilty',\n",
       " 'envious',\n",
       " 'insecure',\n",
       " 'ashamed',\n",
       " 'betrayed',\n",
       " 'disappointed',\n",
       " 'regretful',\n",
       " 'indifferent',\n",
       " 'proud',\n",
       " 'surprised',\n",
       " 'curious',\n",
       " 'ambivalent',\n",
       " 'bored',\n",
       " 'brave',\n",
       " 'concerned',\n",
       " 'creative',\n",
       " 'daring',\n",
       " 'determined',\n",
       " 'discontent',\n",
       " 'ecstatic',\n",
       " 'energetic',\n",
       " 'enthralled',\n",
       " 'enthusiastic',\n",
       " 'fascinated',\n",
       " 'fearful',\n",
       " 'flattered',\n",
       " 'focused',\n",
       " 'great',\n",
       " 'grief',\n",
       " 'helpless',\n",
       " 'humiliated',\n",
       " 'impatient',\n",
       " 'impressed',\n",
       " 'indignant',\n",
       " 'inferior',\n",
       " 'inquisitive',\n",
       " 'intense',\n",
       " 'intrigued',\n",
       " 'jubilant',\n",
       " 'lazy',\n",
       " 'lethargic',\n",
       " 'liberated',\n",
       " 'lost',\n",
       " 'miserable',\n",
       " 'misunderstood',\n",
       " 'mixed up',\n",
       " 'moody',\n",
       " 'motivated',\n",
       " 'nostalgic',\n",
       " 'obligated',\n",
       " 'obsessed',\n",
       " 'optimistic',\n",
       " 'panicked',\n",
       " 'peaceful',\n",
       " 'pessimistic',\n",
       " 'playful',\n",
       " 'rebellious',\n",
       " 'rejuvenated',\n",
       " 'relieved',\n",
       " 'resigned',\n",
       " 'restless',\n",
       " 'revitalized',\n",
       " 'scared',\n",
       " 'self-assured',\n",
       " 'self-conscious',\n",
       " 'selfish',\n",
       " 'sensual',\n",
       " 'sensuous',\n",
       " 'shaken',\n",
       " 'shy',\n",
       " 'sincere',\n",
       " 'skeptical',\n",
       " 'sorry',\n",
       " 'spiritual',\n",
       " 'stubborn',\n",
       " 'submissive',\n",
       " 'successful',\n",
       " 'suffering',\n",
       " 'suspicious',\n",
       " 'sympathetic',\n",
       " 'tense',\n",
       " 'terrified',\n",
       " 'thoughtful',\n",
       " 'admiring',\n",
       " 'aggravated',\n",
       " 'agitated',\n",
       " 'apathetic',\n",
       " 'appreciative',\n",
       " 'awed',\n",
       " 'awkward',\n",
       " 'bitter',\n",
       " 'bold',\n",
       " 'bright',\n",
       " 'contemplative',\n",
       " 'cooperative',\n",
       " 'courageous',\n",
       " 'covetous',\n",
       " 'crushed',\n",
       " 'cynical',\n",
       " 'dancing',\n",
       " 'dear',\n",
       " 'desperate',\n",
       " 'despairing',\n",
       " 'devoted',\n",
       " 'discouraged',\n",
       " 'disheartened',\n",
       " 'distressed',\n",
       " 'distrustful',\n",
       " 'enchanted',\n",
       " 'encouraged',\n",
       " 'expected',\n",
       " 'fanciful',\n",
       " 'fearing',\n",
       " 'fervent',\n",
       " 'fiesty',\n",
       " 'forgiving',\n",
       " 'abundant',\n",
       " 'accepting',\n",
       " 'achy',\n",
       " 'admired',\n",
       " 'adventurous',\n",
       " 'agreeable',\n",
       " 'alarmed',\n",
       " 'alert',\n",
       " 'alienated',\n",
       " 'alive',\n",
       " 'alone',\n",
       " 'aloof',\n",
       " 'amazed',\n",
       " 'animated',\n",
       " 'ardent',\n",
       " 'argumentative',\n",
       " 'assured',\n",
       " 'attacked',\n",
       " 'attracted',\n",
       " 'awestruck',\n",
       " 'awful',\n",
       " 'bad',\n",
       " 'bashful',\n",
       " 'beaten',\n",
       " 'believer',\n",
       " 'boastful',\n",
       " 'bountiful',\n",
       " 'bubbly',\n",
       " 'bullied',\n",
       " 'calculated',\n",
       " 'capable',\n",
       " 'caring',\n",
       " 'cautionary',\n",
       " 'cautious',\n",
       " 'ceremonious',\n",
       " 'certain',\n",
       " 'charismatic',\n",
       " 'charitable',\n",
       " 'cheery',\n",
       " 'chilly',\n",
       " 'clever',\n",
       " 'closed',\n",
       " 'cold',\n",
       " 'comfortable',\n",
       " 'comforted',\n",
       " 'communicative',\n",
       " 'considerate',\n",
       " 'cool',\n",
       " 'defeated',\n",
       " 'deflated',\n",
       " 'dejected',\n",
       " 'despicable',\n",
       " 'dirty',\n",
       " 'disenchanted',\n",
       " 'disgruntled',\n",
       " 'distant',\n",
       " 'distracted',\n",
       " 'dominated',\n",
       " 'dominating',\n",
       " 'downtrodden',\n",
       " 'dreadful',\n",
       " 'drifting',\n",
       " 'eager',\n",
       " 'earnest',\n",
       " 'easy',\n",
       " 'easygoing',\n",
       " 'effective',\n",
       " 'efficient',\n",
       " 'emotional',\n",
       " 'empath',\n",
       " 'empathetic',\n",
       " 'empowering',\n",
       " 'encouraging',\n",
       " 'evenhanded',\n",
       " 'evil',\n",
       " 'exhausted',\n",
       " 'failing',\n",
       " 'fast',\n",
       " 'festive',\n",
       " 'flailing',\n",
       " 'floating',\n",
       " 'fortunate',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'frisky',\n",
       " 'fulfilled',\n",
       " 'gentle',\n",
       " 'genuine',\n",
       " 'giving',\n",
       " 'gloomy',\n",
       " 'glum',\n",
       " 'gracious',\n",
       " 'grieved',\n",
       " 'grieving',\n",
       " 'grumpy',\n",
       " 'guided',\n",
       " 'guiding',\n",
       " 'hardy',\n",
       " 'harassed',\n",
       " 'harsh',\n",
       " 'hateful',\n",
       " 'healthy',\n",
       " 'heard',\n",
       " 'heavy',\n",
       " 'hesitant',\n",
       " 'honest',\n",
       " 'hormonal',\n",
       " 'hostile',\n",
       " 'hungry',\n",
       " 'hurried',\n",
       " 'icky',\n",
       " 'ignored',\n",
       " 'ill',\n",
       " 'impulsive',\n",
       " 'incapable',\n",
       " 'indecisive',\n",
       " 'injured',\n",
       " 'intelligent',\n",
       " 'joyous',\n",
       " 'justified',\n",
       " 'keen',\n",
       " 'knowledgeable',\n",
       " 'lively',\n",
       " 'loved',\n",
       " 'loyal',\n",
       " 'lucky',\n",
       " 'meditative',\n",
       " 'melancholy',\n",
       " 'mellow',\n",
       " 'menaced',\n",
       " 'merry',\n",
       " 'misgiving',\n",
       " 'mysterious',\n",
       " 'negative',\n",
       " 'nonchalant',\n",
       " 'neutral',\n",
       " 'numb',\n",
       " 'obnoxious',\n",
       " 'old',\n",
       " 'open',\n",
       " 'optimal',\n",
       " 'paralyzed',\n",
       " 'passive',\n",
       " 'pathetic',\n",
       " 'perfect',\n",
       " 'perplexed',\n",
       " 'poised',\n",
       " 'poor',\n",
       " 'positive',\n",
       " 'pragmatic',\n",
       " 'pulled',\n",
       " 'pushed',\n",
       " 'puzzled',\n",
       " 'ravenous',\n",
       " 'reassured',\n",
       " 'receptive',\n",
       " 'recovering',\n",
       " 'reliable',\n",
       " 'reluctant',\n",
       " 'respectful',\n",
       " 'reserved',\n",
       " 'rich',\n",
       " 'sadistic',\n",
       " 'sarcastic',\n",
       " 'scarce',\n",
       " 'sensitive',\n",
       " 'sentimental',\n",
       " 'settled',\n",
       " 'shaky',\n",
       " 'shamed',\n",
       " 'sick',\n",
       " 'silent',\n",
       " 'slow',\n",
       " 'smart',\n",
       " 'solemn',\n",
       " 'sorrowful',\n",
       " 'sour',\n",
       " 'spirited',\n",
       " 'spiteful',\n",
       " 'suboptimal',\n",
       " 'sullen',\n",
       " 'sunny',\n",
       " 'supportive',\n",
       " 'sure',\n",
       " 'sweet',\n",
       " 'tearful',\n",
       " 'tenacious',\n",
       " 'tender',\n",
       " 'terrible',\n",
       " 'threatened',\n",
       " 'timid',\n",
       " 'tormented',\n",
       " 'tragic',\n",
       " 'transformed',\n",
       " 'transient',\n",
       " 'trusting',\n",
       " 'uncomfortable',\n",
       " 'understanding',\n",
       " 'uneasy',\n",
       " 'unengaged',\n",
       " 'unfulfilled',\n",
       " 'unsupported',\n",
       " 'unsure',\n",
       " 'untouchable',\n",
       " 'useful',\n",
       " 'useless',\n",
       " 'victimized',\n",
       " 'violent',\n",
       " 'vivacious',\n",
       " 'volatile',\n",
       " 'vulnerable',\n",
       " 'warm',\n",
       " 'weak',\n",
       " 'wealthy',\n",
       " 'weary',\n",
       " 'woeful',\n",
       " 'wonderful',\n",
       " 'young',\n",
       " 'youthful',\n",
       " 'anger',\n",
       " 'love',\n",
       " 'rage',\n",
       " 'torment',\n",
       " 'irritation']"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = feature['Feeling'].tolist()\n",
    "feature['Feeling']\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "92afd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "# create a dictionary to store the mappings between the original words and their cleaned versions\n",
    "word_dict = {}\n",
    "\n",
    "def get_word(word):\n",
    "    if word not in word_dict:\n",
    "        pos = pos_tag([word])[0][1]\n",
    "        if pos.startswith('J'):\n",
    "            clean_word = lemmatizer(word, get_simple_pos(pos))[0]\n",
    "            word_dict[word] = clean_word.lower()\n",
    "        else:\n",
    "            word_dict[word] = \"\"\n",
    "    return word_dict[word]\n",
    "\n",
    "def get_tag(text):\n",
    "    output_words = [get_word(word) for word in text]\n",
    "    return output_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "e897a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_extract(text):\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "#     pos_tags = pos_tag(tokens)  # perform POS tagging on the tokens\n",
    "#     extracted_features = []\n",
    "#     extracted_text.append([tag[1] for tag in pos_tags])\n",
    "#     return extracted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "797a5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweets_description'] = df['Tweets_description'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "fd079217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_sentiment_label</th>\n",
       "      <th>Tweets_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>scheduled morning 2 days fact yesnot sure even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>seeing workers time time going beyond love fly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>flew ord miami back great crew service legs th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>thats horse radish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>flight ord delayed air force one last flight s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260270</th>\n",
       "      <td>neutral</td>\n",
       "      <td>456 crores paid neerav modi recovered congress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260271</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dear rss terrorist payal gawar modi killing 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260272</th>\n",
       "      <td>negative</td>\n",
       "      <td>cover interaction forum left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260273</th>\n",
       "      <td>negative</td>\n",
       "      <td>big project came india modi dream project happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260274</th>\n",
       "      <td>positive</td>\n",
       "      <td>ever listen like gurukul discipline maintained...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_sentiment_label                                 Tweets_description\n",
       "0                  negative  scheduled morning 2 days fact yesnot sure even...\n",
       "1                  positive  seeing workers time time going beyond love fly...\n",
       "2                  positive  flew ord miami back great crew service legs th...\n",
       "3                  negative                                 thats horse radish\n",
       "4                  negative  flight ord delayed air force one last flight s...\n",
       "...                     ...                                                ...\n",
       "260270              neutral  456 crores paid neerav modi recovered congress...\n",
       "260271              neutral  dear rss terrorist payal gawar modi killing 10...\n",
       "260272             negative                       cover interaction forum left\n",
       "260273             negative  big project came india modi dream project happ...\n",
       "260274             positive  ever listen like gurukul discipline maintained...\n",
       "\n",
       "[260275 rows x 2 columns]"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweets_description'] = df['Tweets_description'].apply(clean_tweets)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "169785d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200842\n",
      "170946\n",
      "148762\n"
     ]
    }
   ],
   "source": [
    "print(df[df['text_sentiment_label'] == 'positive'].size)\n",
    "print(df[df['text_sentiment_label'] == 'negative'].size)\n",
    "print(df[df['text_sentiment_label'] == 'neutral'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "478f0e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fatures = 3000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df['Tweets_description'].values)\n",
    "X = tokenizer.texts_to_sequences(df['Tweets_description'].values)\n",
    "X = pad_sequences(X)\n",
    "X[:2]\n",
    "input_len = X.shape[1]\n",
    "input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6676e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "num_classes = 3\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = input_len))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='sigmoid')) # Change activation function to sigmoid\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "93ab6238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_16 (Embedding)    (None, 162, 128)          384000    \n",
      "                                                                 \n",
      " spatial_dropout1d_3 (Spatia  (None, 162, 128)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 196)               254800    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 3)                 591       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 639,391\n",
      "Trainable params: 639,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "num_classes = 3\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = input_len))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "f07e7433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208220, 162) (208220, 3)\n",
      "(52055, 162) (52055, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "label_dict = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "df['label_int'] = df['text_sentiment_label'].replace(label_dict)\n",
    "Y = to_categorical(df['label_int'], num_classes=num_classes)\n",
    "# Y = pd.get_dummies(df['text_sentiment_label']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "16916f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1286/1286 [==============================] - 1322s 1s/step - loss: 0.7011 - accuracy: 0.6991\n",
      "Epoch 2/15\n",
      "1286/1286 [==============================] - 1420s 1s/step - loss: 0.5650 - accuracy: 0.7729\n",
      "Epoch 3/15\n",
      "1286/1286 [==============================] - 2755s 2s/step - loss: 0.5321 - accuracy: 0.7897\n",
      "Epoch 4/15\n",
      "1286/1286 [==============================] - 2536s 2s/step - loss: 0.5081 - accuracy: 0.8008\n",
      "Epoch 5/15\n",
      "1286/1286 [==============================] - 2051s 2s/step - loss: 0.4915 - accuracy: 0.8089\n",
      "Epoch 6/15\n",
      "1286/1286 [==============================] - 2029s 2s/step - loss: 0.4745 - accuracy: 0.8161\n",
      "Epoch 7/15\n",
      "1286/1286 [==============================] - 2014s 2s/step - loss: 0.4591 - accuracy: 0.8222\n",
      "Epoch 8/15\n",
      "1286/1286 [==============================] - 2097s 2s/step - loss: 0.4455 - accuracy: 0.8284\n",
      "Epoch 9/15\n",
      "1286/1286 [==============================] - 2053s 2s/step - loss: 0.4322 - accuracy: 0.8325\n",
      "Epoch 10/15\n",
      "1286/1286 [==============================] - 1971s 2s/step - loss: 0.4191 - accuracy: 0.8377\n",
      "Epoch 11/15\n",
      "1286/1286 [==============================] - 2083s 2s/step - loss: 0.4086 - accuracy: 0.8418\n",
      "Epoch 12/15\n",
      "1286/1286 [==============================] - 2090s 2s/step - loss: 0.3982 - accuracy: 0.8459\n",
      "Epoch 13/15\n",
      "1286/1286 [==============================] - 1969s 2s/step - loss: 0.3888 - accuracy: 0.8495\n",
      "Epoch 14/15\n",
      "1286/1286 [==============================] - 1857s 1s/step - loss: 0.3803 - accuracy: 0.8527\n",
      "Epoch 15/15\n",
      "1286/1286 [==============================] - 2004s 2s/step - loss: 0.3709 - accuracy: 0.8558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28a9b348c10>"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 162\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "0c447b76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1627/1627 [==============================] - 193s 118ms/step\n",
      "confusion matrix [[16389  1660  2142]\n",
      " [  591 15015  1376]\n",
      " [ 1480  1777 11625]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.85     20191\n",
      "           1       0.81      0.88      0.85     16982\n",
      "           2       0.77      0.78      0.77     14882\n",
      "\n",
      "    accuracy                           0.83     52055\n",
      "   macro avg       0.82      0.83      0.82     52055\n",
      "weighted avg       0.83      0.83      0.83     52055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "Y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "ed7502c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1627/1627 [==============================] - 530s 325ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "137027"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = np.argmax(model.predict(X_test))\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "9be0801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_analysis_trail_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "77258fd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[461], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Separate majority and minority classes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_majority \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_sentiment_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m data_minority \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_sentiment_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m bias \u001b[38;5;241m=\u001b[39m data_minority\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mdata_majority\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1530\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "data_majority = df[df['text_sentiment_label'] == 'neutral' or data['sentiment'] == 'positive']\n",
    "data_minority = df[df['text_sentiment_label'] == 'negative']\n",
    "\n",
    "bias = data_minority.shape[0]/data_majority.shape[0]\n",
    "# lets split train/test data first then \n",
    "train = pd.concat([data_majority.sample(frac=0.8,random_state=200),\n",
    "         data_minority.sample(frac=0.8,random_state=200)])\n",
    "test = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),\n",
    "        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('positive data in training:',(train.sentiment == 'positive' or train.sentiment == 'neutral').sum())\n",
    "print('negative data in training:',(train.sentiment == 'negative').sum())\n",
    "print('positive data in test:',(test.sentiment == 'positive' or train.sentiment == 'neutral').sum())\n",
    "print('negative data in test:',(test.sentiment == 'negative').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate majority and minority classes in training data for upsampling \n",
    "data_majority = train[train['sentiment'] == 'Negative']\n",
    "data_minority = train[train['sentiment'] == 'Positive']\n",
    "\n",
    "print(\"majority class before upsample:\",data_majority.shape)\n",
    "print(\"minority class before upsample:\",data_minority.shape)\n",
    "\n",
    "# Upsample minority class\n",
    "data_minority_upsampled = resample(data_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "print(\"After upsampling\\n\",data_upsampled.sentiment.value_counts(),sep = \"\")\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df['Tweets_description'].values) # training with whole data\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(data_upsampled['Tweets_description'].values)\n",
    "X_train = pad_sequences(X_train,maxlen=29)\n",
    "Y_train = pd.get_dummies(data_upsampled['text_sentiment_label']).values\n",
    "print('x_train shape:',X_train.shape)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test['Tweets_description'].values)\n",
    "X_test = pad_sequences(X_test,maxlen=29)\n",
    "Y_test = pd.get_dummies(test['sentiment']).values\n",
    "print(\"x_test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "embed_dim = 128\n",
    "lstm_out = 192\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# also adding weights\n",
    "class_weights = {0: 1 ,\n",
    "                1: 1.6/bias }\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "          class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running model to few more epochs\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "          class_weight=class_weights)\n",
    "Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "twt = ['keep up the good work']\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=29, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"negative\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f3e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ae861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec1efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12390a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "cd797bc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[407], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# running model to few more epochs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m class_weight\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m----> 4\u001b[0m           class_weight\u001b[38;5;241m=\u001b[39m\u001b[43mclass_weights\u001b[49m)\n\u001b[0;32m      5\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_classes(X_test,batch_size \u001b[38;5;241m=\u001b[39m batch_size)\n\u001b[0;32m      6\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m: Y_test\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m:Y_pred})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_weights' is not defined"
     ]
    }
   ],
   "source": [
    "# running model to few more epochs\n",
    "from sklearn.utils import class_weight\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,\n",
    "          class_weight=class_weights)\n",
    "Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b4335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "cf7c5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = []\n",
    "test_pos = []\n",
    "for text in train_data:\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    train_pos.append([tag[1] for tag in pos_tags])\n",
    "\n",
    "for text in test_data:\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    test_pos.append([tag[1] for tag in pos_tags])\n",
    "\n",
    "pos_dict = {'CC': 1, 'CD': 2, 'DT': 3, 'EX': 4, 'FW': 5, 'IN': 6, 'JJ': 7, 'JJR': 8, 'JJS': 9, 'LS': 10,\n",
    "            'MD': 11, 'NN': 12, 'NNS': 13, 'NNP': 14, 'NNPS': 15, 'PDT': 16, 'POS': 17, 'PRP': 18, 'PRP$': 19,\n",
    "            'RB': 20, 'RBR': 21, 'RBS': 22, 'RP': 23, 'SYM': 24, 'TO': 25, 'UH': 26, 'VB': 27, 'VBD': 28,\n",
    "            'VBG': 29, 'VBN': 30, 'VBP': 31, 'VBZ': 32, 'WDT': 33, 'WP': 34, 'WP$': 35, 'WRB': 36, '$' : 37}\n",
    "\n",
    "# Convert the POS tags to their corresponding numeric values\n",
    "train_pos_num = [[pos_dict[tag] for tag in pos_tags] for pos_tags in train_pos]\n",
    "test_pos_num = [[pos_dict[tag] for tag in pos_tags] for pos_tags in test_pos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "c8548262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature extraction\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# X = vectorizer.fit_transform(df['Text'])\n",
    "# X_array = X.toarray()\n",
    "# df['Text'] = list(X_array)\n",
    "# df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ee96951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df['X_train'] = df['Text']\n",
    "# label_dict = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "# Y_label = df['text_sentiment_label'].tolist()\n",
    "# Y = [label_dict[label] for label in Y_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "11bb2f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_10/embedding_11/embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_15956\\2932057558.py\", line 34, in <module>\n      model.fit(train_pos_pad, train_labels_num, epochs=10, batch_size=32, validation_split=0.2)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'sequential_10/embedding_11/embedding_lookup'\nindices[0,0] = 12 is not in [0, 1)\n\t [[{{node sequential_10/embedding_11/embedding_lookup}}]] [Op:__inference_train_function_23068]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[334], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pos_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_analysis1_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_10/embedding_11/embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_15956\\2932057558.py\", line 34, in <module>\n      model.fit(train_pos_pad, train_labels_num, epochs=10, batch_size=32, validation_split=0.2)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\lenovo\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'sequential_10/embedding_11/embedding_lookup'\nindices[0,0] = 12 is not in [0, 1)\n\t [[{{node sequential_10/embedding_11/embedding_lookup}}]] [Op:__inference_train_function_23068]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_pos_num = np.array(train_pos_num)\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "max_length = max([len(seq) for seq in train_pos_num])\n",
    "max_length_1 = max([len(seq) for seq in test_pos_num])\n",
    "max_length = max(max_length, max_length_1)\n",
    "train_pos_pad = pad_sequences(train_pos_num, maxlen=max_length, padding='post')\n",
    "test_pos_pad = pad_sequences(test_pos_num, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert your labels to numeric values\n",
    "label_dict = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "train_labels_num = [label_dict[label] for label in train_labels]\n",
    "test_labels_num = [label_dict[label] for label in test_labels]\n",
    "train_labels_num = np.array(train_labels_num)\n",
    "# Define your LSTM model\n",
    "model = models.Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=16, input_length=max_length),\n",
    "    Flatten(),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# Train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_pos_pad, train_labels_num, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the model\n",
    "model.save('sentiment_analysis1_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Define a function to convert a sentence to a feature vector\n",
    "# def sentence_to_vector(sentence, model):\n",
    "#     words = sentence.split()\n",
    "#     vectors = [model[word] for word in words if word in model]\n",
    "#     if len(vectors) == 0:\n",
    "#         return np.zeros(model.vector_size)\n",
    "#     else:\n",
    "#         return np.mean(vectors, axis=0)\n",
    "\n",
    "# # Example usage\n",
    "# sentence = \"This is a sample sentence for feature extraction\"\n",
    "# vector = sentence_to_vector(sentence, model)\n",
    "# print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "# MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# # Train the Word2Vec model\n",
    "# sentences = [[\"this\", \"is\", \"the\", \"first\", \"sentence\", \"for\", \"word2vec\"],\n",
    "#              [\"this\", \"is\", \"the\", \"second\", \"sentence\"],\n",
    "#              [\"yet\", \"another\", \"sentence\"],\n",
    "#              [\"one\", \"more\", \"sentence\"],\n",
    "#              [\"and\", \"the\", \"final\", \"sentence\"]]\n",
    "# model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# # Define the Keras model\n",
    "# vocab_size = len(model.wv.key_to_index)\n",
    "\n",
    "# embedding_size = model.vector_size\n",
    "# model_keras = Sequential([\n",
    "#     Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=MAX_SEQUENCE_LENGTH,\n",
    "#               weights=[model.wv.vectors], trainable=False),\n",
    "#     Flatten(),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile and train the Keras model\n",
    "# model_keras.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model_keras.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c178a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# svcv = SVC(C=0.2,kernel='linear')\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(df['X_train'], df['Y_train'], test_size=0.2, random_state=42)\n",
    "# # Convert the text to sequences of integers\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# svcv.fit(X_train_seq, Y_train)\n",
    "# svcv_scr=svcv.score(X_test_seq, Y_test)\n",
    "# svcv_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095df57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629be909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc18e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
