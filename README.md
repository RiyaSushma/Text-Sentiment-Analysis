# Text-Sentiment-Analysis
Sentiment analysis plays a vital role to analyse written text which can give idea about the author’s attitude toward a particular topic, movie, product, etc. Text Sentiment Analysis is a collection of methodologies that classify the polarity of a given text in a document, sentence, or word as whether it is positive, negative, or neutral response. Such systems can be widely used to provide real-time insights into the sentiments of people which can be helpful in Product analysis, Customer service Response, Social media monitoring and brand management. In this study we attempt to detect underlying sentiments by using Natural Language Processing and text mining to identify the emotional information from text materials. In this project, we will predict the sentiments of text in the given dataset using RNN and machine learning algorithms. The dataset consists of reviews taken from different sites it contains pre-processed data which consists of one line per document and the label (positive, negative) is at the end of each line which describe different sentiments like positive, negative or neutral. The major goal of the proposed system is understanding Recurrent Neural Network, and predicting sentiments based on model. This code uses Neural Networks: The overall architecture of Neural Network comprises of a number of neurons and activation unit, this circuit of units serves their function of finding underlying relationships in the data.

We are using is Multi-class Sentiment datasets containing product reviews taken from Amazon, Tweets, etc., to train and validate our models. In total, these datasets contain unprocessed, processed, processed data with number of stars included. The processed data is one line per document with each line in format (feature:<count>….feature:<count>#label#:<label>. Label is always at the end of the line which describe different sentiments like positive, negative or neutral.

A. Data Cleaning and Text Normalisation (To extract only useful data from summary) Step 1: Remove stop words like as, and, or, these words does not affect the Output which is to classify the text on the basis of its sentiments Step 2: Removing all symbols (@mentions) and emoji. This process is done by finding patterns, like for mentions, ‘@text’, by using Regular Expressions (It is used to detect if a particular pattern is present in a string and text or not), and replacing them with empty string.
Step 3: Word Tokenization: Here, word_tokenize() which is NLTK library's function is used to tokenize or split the tweet into individual words. Tokenization is the process of breaking down a piece of text into smaller parts, such as words or phrases, for further analysis or processing. In this case, the tweet is being broken down into a list of individual words. Step 4: Lemmatization: It converts the words into their meaningful base form, termed as Lemma. For example, for ‘caring’ lemma is ‘care’. Here in Sentiment Analysis getting base word is important to know whether the word is positive or negative. Stemming and lemmatization is used to get that base word. Further Processing:
I. Categorical Encoding
For any structured dataset including multiple columns, which are a combination of numerical and categorical columns. It is a necessary step to convert categorical columns to numerical columns as machine understand only numbers and not text. This process of converting categorical columns to numerical ones is called Categorical Encoding.
In this code, the sentiment labels are converted to one-hot encoding using a dictionary mapping each sentiment to its corresponding one-hot encoding vector. Then, the labels are further converted to numpy arrays and the training and testing data are split into train and test sets. Finally, the labels are converted to one-hot encoding vectors using the label_to_onehot dictionary.
One-hot encoding is a technique used in Machine Learning to convert or represent categorical data as numerical data. In one-hot encoding, each category is represented as a binary vector, where each element in the vector corresponds to a possible category value. For example, in this code we have three categories, Positive, Negative and Neutral, therefore the one-hot encoding for category Positive is [1, 0, 0], for category negative is [0, 1, 1], and for category Neutral is [0, 0, 1].
The advantage of one-hot encoding is that it allows to use categorical data (by converting it to numerical one) in machine learning models which are solely designed to work with numerical data. It also ensures that the categorical data is treated as distinct and independent values, rather than assigning arbitrary numerical values that might introduce unwanted relationships or bias into the data.
II. Training and Test Data Splitting
The train_test_split() function is used to split the data into two sets: a training set and a testing set. This function randomly divides the data into two sets based on the specified test size.
In the code, the data is split using a test size of 0.2, which means that 20% of the data will be used for testing, while 80% will be used for training. The X_train and Y_train variables represent the training data. The 'df' object is assumed to be a pandas Data Frame that has two columns: 'X_train', which contains the text data, and 'Y_train', which contains the corresponding sentiment labels. The train_test_split() function is called with df['X_train'] and df['Y_train'] as input arguments. The function then returns four variables: X_train, X_test, Y_train, and Y_test. The X_train and Y_train variables represent the training data, which will be used to train the model. The X_test and Y_test variables represent the testing data, which will be used to evaluate the performance of the trained model. The random_state parameter is set to 42, which means that the data will be split in a reproducible manner. This is useful for ensuring that the results are consistent across different runs of the code.
III. Text to sequence of Converting Integers
Tokenization is the process of breaking a large text into smaller pieces such as individual words or terms. The Tokenizer class in Tensor Flow’s Keras API is a text pre-processing module that is used to prepare text for use with deep learning models. It converts text into a sequence of integers, where each integer corresponds to a specific word in the text. This is useful because most deep learning models work with numerical data rather than text.
In the code provided, the Tokenizer is first initialized with the num_words parameter (he num_words parameter in the Tokenizer class specifies the maximum number of words to keep in the vocabulary. The Tokenizer class keeps track of all the unique words that it encounters in the text data, and it assigns a unique integer index to each word. By setting the num_words parameter to a value, the Tokenizer class will only keep the most frequent num_words words in the vocabulary, and all other words will be ignored. So, the num_words parameter is used to limit the size of the vocabulary and reduce the computational complexity of the model.) set to 10000, which limits the number of words that will be considered during tokenization. The fit_on_texts() method is then called on the training data to prepare the Tokenizer to convert the text data into a sequence of integers. Finally, the texts_to_sequences() method is used to convert the text into
sequences of integers, which can be used as input to the deep learning model.

The Tokenizer class also has other useful features such as allowing you to filter out specific characters or words, or to limit the vocabulary size, which can be useful in text pre-processing for deep learning models.
The pad_sequences() function is used to ensure that all sequences in a list have the same length by padding them with zeros or truncating them. This is important because neural networks require inputs to have the same shape. In the code, the function is used to pad the sequences to the same length as the longest sequence in the training set.
Specifically, the pad_sequences() function takes in the sequences and the maximum length of the sequences and pads them with zeros or truncates them to that length. In the code, max_length is the maximum length of the sequences in the training set, which is computed as the length of the longest sequence in X_train_seq. The padding is added to the end of each sequence, indicated by the padding='post' parameter.
IV. Tensorflow
tf.keras.Sequential: defines the neural network architecture, creates a simple feedforward neural network with three layers: an input layer with 32 units, a hidden layer with 16 units, and an output layer with 1 unit. The input_dim parameter of the first layer is set to X_train.shape[1], which is the number of features in the training data, so the input layer has the appropriate number of units. Categorical cross-entropy is a loss function that is commonly used for multi-class classification problems. In the case of sentiment analysis, it is used to measure the difference between the predicted probability distribution (output of the neural network) and the true probability distribution (the actual labels of the training data).

where y is the true probability distribution (i.e., the one-hot encoded labels) and ŷ is the predicted probability distribution. The formula takes the sum of the element-wise product of y and the natural logarithm of ŷ, and then multiplies by -1 to obtain the loss. The goal of the training process is to minimize this loss, which is accomplished by adjusting the weights and biases of the neural network through backpropagation.
The optimizer is set to 'adam', which is a popular optimization algorithm for neural networks. Finally, the model is set to compute accuracy as a metric during training.
After training, the model is evaluated using the evaluate method. The test data (X_test and y_test) are passed as arguments, and the method returns the loss and accuracy of the model on the test data. V. LSTM (Long Short-Term Memory) Long short-term memory (LSTM) is an extension of recurrent neural network (RNN) (as they extend the memory) architecture employed in building applications based on deep learning. LSTM has feedback connections. There networks are well-suited to classifying, processing, and making predictions based on time series data as there is possibility of lags of unknown duration between important events in time series. LSTM is used for Text Sentiment Analysis, Long short-term memory (LSTM) which is a synthetic recurrent neural network (RNN) architecture employed in the sphere of machine learning. LSTM has feedback connections. When training using traditional RNNs the exploding and vanishing gradient problems may arise which can be accommodated by LSTM as it has relative insensitivity to gap length, which makes it more beneficial from Traditional RNNs, hidden
Markov models, and other sequence learning methods. Out of several architectures of LSTM units typical architecture consists of a cell (the memory a part of the LSTM unit) and three” regulators", usually referred to as gates [2], that regulates the flow of knowledge inside the LSTM unit, they are input gate, an output gate, and a forget gate. Some LSTM unit variations do not have one or more of those gates or may even produce other gates, for example, gated recurrent units (GRUs) don't have an output gate [3]. The unit parameter is set to 32, which means that the LSTM layer will have 32 memory cells (i.e., hidden units). The LSTM layer will take the output of the Embedding layer as input. This line adds a Dense layer to the model. It is a fully connected layer, here, each neuron is connected to every neuron in the previous layer. Finally, we close the list of layers passed to the Sequential model constructor. So, the overall architecture of this model consists of an Embedding layer followed by an LSTM layer and a Dense layer. The Embedding layer maps each word in the input sequences to a dense vector, which is then fed to the LSTM layer to capture the temporal dependencies in the sequence. The output of the LSTM layer is then passed through the Dense layer, which produces a probability distribution over the output classes. Other layers are:  Embedding layer: This layer learns a vector representation for each word in the vocabulary. The input_dim parameter specifies the size of the vocabulary, and the output_dim parameter specifies the size of the embedding vector for each word. The input_length parameter specifies the length of each input sequence (i.e., the number of words in each tweet).  Dense layer with softmax activation: This layer is the output layer of the model. It contains three units, one for each possible sentiment label (negative, neutral, positive). The softmax activation function ensures that the output values are probabilities that sum to one.  The softmax activation function is a commonly used activation function in neural networks, especially for multi-class classification problems. It is a type of exponential function that maps a vector of arbitrary real-valued numbers to a probability distribution. The output of the softmax function is a vector of the same dimension as the input, and each element of the output represents the probability of the corresponding class. The sum of all the elements in the output vector is equal to 1.0, which makes it suitable for multi-class classification problems. where x is the input vector, n is the number of classes, xi is the i-th element of the input vector, and e is the mathematical constant e (2.71828...). VI. Training Model Training the model: The code trains the model using the fit() method. The X_train_pad and Y_train variables are used as input, and the model is trained for 10 epochs with a batch size of 32. The validation split parameter is set to 0.2, which means that 20% of the training data is used for validation. The fit method returns a History object, which contains the training and validation loss and accuracy for each epoch. This object can be used to visualize the training and validation curves and to diagnose any issues with the model (such as overfitting or under fitting). After training, the model is evaluated on the test set using the evaluate method, which returns the loss and accuracy of the model on the test set. Now here, during training, the weights are adjusted so as to minimize the loss function and improve the accuracy of the model on the training data. Once the model is trained, it can
be saved to a file (in this case, a .pkl file) and used for making predictions on new data. After this pkl file for this model is loaded. Saving the model: The code saves the trained model to a file named sentiment_analysis_model.h5 using the save() method of the model object. This allows the model to be loaded and used later for making predictions. VII. TextBlob (Used in text.py file) TextBlob uses natural language processing techniques to perform sentiment analysis. It first tokenizes the text into individual words and applies part-of-speech (POS) tagging (POS (part-of-speech) tagging using its inbuilt POS tagger, which is based on the popular Natural Language Toolkit (NLTK) library. The POS tagger analyses each word in a given text and assigns it a part-of-speech tag based on its context and usage in the sentence. For example, the word "run" can be used as a noun or a verb in different contexts, and TextBlob's POS tagger is designed to identify such nuances and assign the appropriate tag to each word. Once the POS tagging is done, TextBlob can use this information to perform various text analysis tasks, such as sentiment analysis, noun phrase extraction, and more. Some tags are: NNP stands for proper noun, singular; VBZ stands for verb, third person singular present; VBG stands for verb, gerund or present participle; DT stands for determiner; JJ stands for adjective; and NN stands for noun, singular or mass.) getPolarity() function takes a string of text as input and uses TextBlob to calculate the polarity and subjectivity of the text. Polarity refers to the emotional tone of the text, whether it is positive, negative or neutral. Subjectivity refers to how subjective or objective the text is, whether it is an opinion or a fact. getPolarity() returns a tuple with three values: the polarity score (a float value between -1 and 1), the subjectivity score (a float value between 0 and 1) and the sentiment label (either "Positive", "Negative" or "Neutral"). The getSentiments() function uses the getPolarity() function to get the polarity score, subjectivity score and sentiment label of the input text. It then displays these values using st.metric() function, along with an image that corresponds to the sentiment label. VIII. Streamlit (For building web application) Streamlit is an open-source Python library used for building interactive web applications with simple Python scripts. It makes it easy to build and deploy machine learning models, data visualizations, and other types of applications with minimal coding effort. Streamlit.components is a module within Streamlit that allows users to add custom HTML and JavaScript components to their Streamlit app. This can be useful for integrating external visualizations, UI elements, or other features that may not be natively available in Streamlit. IX. For Data Extraction snscrape library to scrape tweets from Twitter based on the user's input, start, and end date. It creates a Pandas dataframe to store the scraped data and saves it as a CSV file with the name of the user's input.

